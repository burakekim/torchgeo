{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "929fa39f-25a1-4c23-9695-916a6fddae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c3078-e347-43b3-8944-315da68f6f30",
   "metadata": {},
   "source": [
    "## Land Cover Classification: Sentinel-2 with EuroCrops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf35e78-47ea-4c0c-ac80-a7dd057e2032",
   "metadata": {},
   "source": [
    "This tutorial will walk you through the process of performing land cover classification with Sentinel-2 satellite imagery, annotated with EuroCrops, using TorchGeo library. We will start with downloading Sentinel-2 data, setting it up with EuroCrops for pixel-wise supervised classification, training a segmentation model, and finally running inference to make sense of Sentinel-2 imagery. \n",
    "\n",
    "Whether you are a remote sensing enthusiast or just curious about deep learning for geospatial data, this guide has something cool for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64fc3dad-6908-4b2c-b714-26b2d6a02722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import planetary_computer\n",
    "import pystac\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.datasets import RasterDataset\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a359-5351-4b3a-8ce6-b3e582f395a3",
   "metadata": {},
   "source": [
    "### 1- Downloading Sentinel-2 Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b7d63-d081-4a8d-b936-b5fdf7d0113a",
   "metadata": {},
   "source": [
    "Fetch Sentinel-2 imagery using Microsoft Planetary Computer and ensure you have the data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3af9815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sentinel2l2a01.blob.core.windows.net/sentinel2-l2/34/U/EA/2022/08/27/S2A_MSIL2A_20220827T093601_N0400_R036_T34UEA_20220829T151158.SAFE/GRANULE/L2A_T34UEA_A037499_20220827T093559/IMG_DATA/R10m/T34UEA_20220827T093601_B02_10m.tif?st=2024-12-04T20%3A45%3A31Z&se=2024-12-05T21%3A30%3A31Z&sp=rl&sv=2024-05-04&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2024-12-05T14%3A26%3A18Z&ske=2024-12-12T14%3A26%3A18Z&sks=b&skv=2024-05-04&sig=hFT16uqd5VjWtWkBhyMx67OSoIIpCk%2BQ25A7Cq4g7B8%3D to /data/sentinel/T34UEA_20220827T093601_B02_10m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209459745/209459745 [00:03<00:00, 64877956.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sentinel2l2a01.blob.core.windows.net/sentinel2-l2/34/U/EA/2022/08/27/S2A_MSIL2A_20220827T093601_N0400_R036_T34UEA_20220829T151158.SAFE/GRANULE/L2A_T34UEA_A037499_20220827T093559/IMG_DATA/R10m/T34UEA_20220827T093601_B03_10m.tif?st=2024-12-04T20%3A45%3A31Z&se=2024-12-05T21%3A30%3A31Z&sp=rl&sv=2024-05-04&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2024-12-05T14%3A26%3A18Z&ske=2024-12-12T14%3A26%3A18Z&sks=b&skv=2024-05-04&sig=hFT16uqd5VjWtWkBhyMx67OSoIIpCk%2BQ25A7Cq4g7B8%3D to /data/sentinel/T34UEA_20220827T093601_B03_10m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217850772/217850772 [00:03<00:00, 72323220.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sentinel2l2a01.blob.core.windows.net/sentinel2-l2/34/U/EA/2022/08/27/S2A_MSIL2A_20220827T093601_N0400_R036_T34UEA_20220829T151158.SAFE/GRANULE/L2A_T34UEA_A037499_20220827T093559/IMG_DATA/R10m/T34UEA_20220827T093601_B04_10m.tif?st=2024-12-04T20%3A45%3A31Z&se=2024-12-05T21%3A30%3A31Z&sp=rl&sv=2024-05-04&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2024-12-05T14%3A26%3A18Z&ske=2024-12-12T14%3A26%3A18Z&sks=b&skv=2024-05-04&sig=hFT16uqd5VjWtWkBhyMx67OSoIIpCk%2BQ25A7Cq4g7B8%3D to /data/sentinel/T34UEA_20220827T093601_B04_10m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219170487/219170487 [00:03<00:00, 67522033.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://sentinel2l2a01.blob.core.windows.net/sentinel2-l2/34/U/EA/2022/08/27/S2A_MSIL2A_20220827T093601_N0400_R036_T34UEA_20220829T151158.SAFE/GRANULE/L2A_T34UEA_A037499_20220827T093559/IMG_DATA/R10m/T34UEA_20220827T093601_B08_10m.tif?st=2024-12-04T20%3A45%3A31Z&se=2024-12-05T21%3A30%3A31Z&sp=rl&sv=2024-05-04&sr=c&skoid=9c8ff44a-6a2c-4dfb-b298-1c9212f64d9a&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2024-12-05T14%3A26%3A18Z&ske=2024-12-12T14%3A26%3A18Z&sks=b&skv=2024-05-04&sig=hFT16uqd5VjWtWkBhyMx67OSoIIpCk%2BQ25A7Cq4g7B8%3D to /data/sentinel/T34UEA_20220827T093601_B08_10m.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232486630/232486630 [00:03<00:00, 65934489.32it/s]\n"
     ]
    }
   ],
   "source": [
    "root = \"/data/sentinel\"\n",
    "item_urls = [\n",
    "    'https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2A_MSIL2A_20220827T093601_R036_T34UEA_20220829T151158',\n",
    "]\n",
    "for item_url in item_urls:\n",
    "    item = pystac.Item.from_file(item_url)\n",
    "    signed_item = planetary_computer.sign(item)\n",
    "    for band in ['B02', 'B03', 'B04', 'B08']:\n",
    "        asset_href = signed_item.assets[band].href\n",
    "        filename = urlparse(asset_href).path.split('/')[-1]\n",
    "        download_url(asset_href, root, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c64ded-bea4-4dcc-aefe-104cc57a0e74",
   "metadata": {},
   "source": [
    "### 2-  Prepare Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcb27b",
   "metadata": {},
   "source": [
    "Customize TorchGeo to align `Sentinel2` and `EuroCrops` datasets, forming an `IntersectionDataset` for pixel-wise classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e88380a5-0a47-4ace-8171-657a9d688164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting EuroCrops CRS from EPSG:4326 to EPSG:32615\n",
      "Converting EuroCrops res from 1e-05 to 10\n",
      "Converting EuroCrops CRS from EPSG:4326 to EPSG:32615\n",
      "Converting EuroCrops res from 1e-05 to 10\n",
      "Converting EuroCrops CRS from EPSG:4326 to EPSG:32615\n",
      "Converting EuroCrops res from 1e-05 to 10\n"
     ]
    }
   ],
   "source": [
    "from torchgeo.datasets import Sentinel2\n",
    "\n",
    "class Sentinel2Custom(Sentinel2):\n",
    "    filename_glob = 'T*_B08_10m.tif'\n",
    "    filename_regex = r'.*'\n",
    "    date_format = '%Y%m%dT%H%M%S'\n",
    "    is_image = True\n",
    "    separate_files = True\n",
    "    all_bands = ('B02', 'B03', 'B04', 'B08')\n",
    "    rgb_bands = ('B04', 'B03', 'B02')\n",
    "\n",
    "import torchgeo.datasets\n",
    "torchgeo.datasets.Sentinel2 = Sentinel2Custom\n",
    "\n",
    "from torchgeo.datamodules import Sentinel2EuroCropsDataModule\n",
    "\n",
    "datamodule = Sentinel2EuroCropsDataModule(\n",
    "    eurocrops_paths=\"/data/datatorchgeo\",\n",
    "    sentinel2_paths=\"/data/sentinel\",\n",
    "    batch_size=1,\n",
    "    patch_size=64,\n",
    "    length=8,\n",
    ")\n",
    "\n",
    "datamodule.setup(\"fit\")\n",
    "train_dataset = datamodule.train_dataset\n",
    "datamodule.setup(\"validate\")\n",
    "val_dataset = datamodule.val_dataset\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "\n",
    "# datamodule.val_sampler.length = 3\n",
    "# datamodule.test_sampler.length = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485655d-ea8b-41c1-a10d-5a62bf0ed24a",
   "metadata": {},
   "source": [
    "### 3- Training Semantic Segmentation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6697dde8-fce2-4cc1-b661-7efc3c879307",
   "metadata": {},
   "source": [
    "Train a UNet model with Sentinel-2 images paired with EuroCrops labels to classify land cover, powered by PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2d929e3-4898-47e8-83a1-d7d110dcf42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/ood/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "INFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "INFO: \n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | Unet             | 32.5 M\n",
      "1 | criterion     | CrossEntropyLoss | 0     \n",
      "2 | train_metrics | MetricCollection | 0     \n",
      "3 | val_metrics   | MetricCollection | 0     \n",
      "4 | test_metrics  | MetricCollection | 0     \n",
      "---------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.174   Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | Unet             | 32.5 M\n",
      "1 | criterion     | CrossEntropyLoss | 0     \n",
      "2 | train_metrics | MetricCollection | 0     \n",
      "3 | val_metrics   | MetricCollection | 0     \n",
      "4 | test_metrics  | MetricCollection | 0     \n",
      "---------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.174   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting EuroCrops CRS from EPSG:4326 to EPSG:32615\n",
      "Converting EuroCrops res from 1e-05 to 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb004a83b1b4d7d894e9d554765e446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "from torchgeo.trainers import SemanticSegmentationTask\n",
    "from lightning.pytorch import Trainer\n",
    "import torch\n",
    "\n",
    "task = SemanticSegmentationTask(\n",
    "    model='unet',\n",
    "    backbone='resnet50',\n",
    "    weights=None,  # No pre-trained weights\n",
    "    in_channels=4,  # CDL dataset may have RGB inputs\n",
    "    num_classes=134,\n",
    "    num_filters=3,\n",
    "    loss='ce',  # CrossEntropyLoss\n",
    "    class_weights=None,\n",
    "    ignore_index=None,\n",
    "    lr=0.001,\n",
    "    patience=10,\n",
    "    freeze_backbone=False,\n",
    "    freeze_decoder=False)\n",
    "\n",
    "# accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cpu',\n",
    "    default_root_dir='./',\n",
    "    fast_dev_run=True,\n",
    "    log_every_n_steps=1,\n",
    "    min_epochs=1,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "trainer.fit(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8471b-9819-4455-9555-883ca14c02cb",
   "metadata": {},
   "source": [
    "### 4- Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff6099-6166-46fd-a17e-8add9f8e66bd",
   "metadata": {},
   "source": [
    "Evaluate the segmentation model over the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc63948-48c8-410a-8138-b52304de95a9",
   "metadata": {},
   "source": [
    "### 5- Inference over full Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba64aa3-225f-4cfd-86e9-e93be5de03d8",
   "metadata": {},
   "source": [
    "Perform inference by computing predictions over the complete imagery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae874d36-f130-47b6-a3e9-9c83d2bd1f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0192d35-0616-432e-b40b-3bed570637a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26d007-4183-4dcd-a58b-a8ae6c886711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "427b239a-3ae8-43b7-9dc7-56f26f81b13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 6, 6)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fe9029b-5224-4ff7-8a81-8abfa50a2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/sentinel/T15TWG_20241101T170349_B02_10m.tif',\n",
       " '/data/sentinel/T34UEA_20220827T093601_B02_10m.tif']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.sentinel2.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b063ee64-ba7b-4a57-8171-b7b83c26a5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/datatorchgeo/AT_2021/AT_2021_EC21.shp',\n",
       " '/data/datatorchgeo/BE_VLG_2021/BE_VLG_2021_EC21.shp',\n",
       " '/data/datatorchgeo/DE_LS_2021/DE_LS_2021_EC21.shp',\n",
       " '/data/datatorchgeo/DE_NRW_2021/DE_NRW_2021_EC21.shp',\n",
       " '/data/datatorchgeo/DK_2019_EC21.shp',\n",
       " '/data/datatorchgeo/EE_2021_EC21.shp',\n",
       " '/data/datatorchgeo/FR_2018/FR_2018_EC21.shp',\n",
       " '/data/datatorchgeo/HR/HR_2020_EC21.shp',\n",
       " '/data/datatorchgeo/LT_2021_EC.shp',\n",
       " '/data/datatorchgeo/LV_2021/LV_2021_EC21.shp',\n",
       " '/data/datatorchgeo/NA/ES_NA_2020_EC21.shp',\n",
       " '/data/datatorchgeo/NL_2020_EC21.shp',\n",
       " '/data/datatorchgeo/PT_2021_EC21.shp',\n",
       " '/data/datatorchgeo/SE/SE_2021_EC21.shp',\n",
       " '/data/datatorchgeo/SI_2021_EC21.shp',\n",
       " '/data/datatorchgeo/SK_2021_EC21.shp']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.eurocrops.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e64d788-4c31-4244-ba38-21089f42db0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(minx=4767668.208980892, maxx=4852845.369973785, miny=12059856.293102827, maxy=12244745.151673885, mint=1609459200.0, maxt=1640995199.999999)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8eec860-23e4-4a5e-8dc8-8a53f4a5565d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(minx=499980.0, maxx=4852845.369973785, miny=4590240.0, maxy=12244745.151673885, mint=0.0, maxt=9.223372036854776e+18)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.sentinel2.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd6709f3-3424-42a5-aff6-7b61864c3e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(minx=2618755.4445662247, maxx=7395571.480706856, miny=9066700.060796153, maxy=12531048.828763735, mint=1514764800.0, maxt=1640995199.999999)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.eurocrops.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21cc2af8-9294-4309-8f8a-46ea260cc9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Sentinel2Custom at 0x7fb97c8cfa10>,\n",
       " <torchgeo.datasets.eurocrops.EuroCrops at 0x7fb97e677610>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a84ecae2-ecf8-4b3a-915f-03c3e28f7e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchgeo.datasets.geo.IntersectionDataset at 0x7fb97e673b10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82c12a25-e47f-44d7-a45a-4056b156e601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchgeo.datasets.geo.IntersectionDataset at 0x7fb97cb20f10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "344a02e7-03c3-4011-8ce1-1f0787ef2fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchgeo.datasets.geo.IntersectionDataset at 0x7fb97cb20f10>, 52, 6, 6)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset, len(datamodule.train_dataset), len(datamodule.val_dataset), len(datamodule.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ef300ee-af65-4735-900b-21154a495f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_batch_sampler.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaffb54b-461d-49d9-94aa-de96af210b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526794950433.0406\n",
      "51743675069.350876\n",
      "221599321761.17267\n",
      "141888810541.07678\n",
      "348908610014.32117\n",
      "221392276629.58197\n",
      "2980960657471.9956\n",
      "700393908177.45\n",
      "269523305955.8701\n",
      "311876677510.05255\n",
      "52470208769.82907\n",
      "173922024020.19308\n",
      "443179357557.58044\n",
      "2605366275475.8315\n",
      "127016410374.98985\n",
      "295224872828.9651\n"
     ]
    }
   ],
   "source": [
    "from torchgeo.datasets.utils import BoundingBox\n",
    "\n",
    "for i in datamodule.eurocrops.index.intersection(datamodule.eurocrops.index.bounds, objects=True):\n",
    "    box1 = BoundingBox(*i.bounds)\n",
    "    print(box1.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86fbc6ee-f34a-4862-8705-fdb557658dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12056040000.0\n",
      "34183890023.70891\n"
     ]
    }
   ],
   "source": [
    "for i in datamodule.sentinel2.index.intersection(datamodule.sentinel2.index.bounds, objects=True):\n",
    "    box1 = BoundingBox(*i.bounds)\n",
    "    print(box1.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43fb640a-c6c6-48a5-9aee-098922eb9457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8cfbf0>\n",
      "hit1 <rtree.index.Item object at 0x7fb97b8abbf0>\n",
      "box1.area 295224872828.9651\n",
      "box2.area 34183890023.70891\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for hit1 in datamodule.eurocrops.index.intersection(datamodule.eurocrops.index.bounds, objects=True):\n",
    "    print(\"hit1\", hit1)\n",
    "    for hit2 in datamodule.sentinel2.index.intersection(hit1.bounds, objects=True):\n",
    "        box1 = BoundingBox(*hit1.bounds)\n",
    "        box2 = BoundingBox(*hit2.bounds)\n",
    "        print(\"box1.area\", box1.area)\n",
    "        print(\"box2.area\", box2.area)\n",
    "        box3 = box1 & box2\n",
    "        # Skip 0 area overlap (unless 0 area dataset)\n",
    "        if box3.area > 0 or box1.area == 0 or box2.area == 0:\n",
    "            # self.index.insert(i, tuple(box3))\n",
    "            i += 1\n",
    "\n",
    "if i == 0:\n",
    "    raise RuntimeError('Datasets have no spatiotemporal intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aae0b348-40f9-46d1-a9d6-3c889793cc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_batch_sampler.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e88e8a34-f998-466d-a2c0-7077be028dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchgeo.datasets.geo.IntersectionDataset at 0x7fb97cb20f10>, 64, 1, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset, datamodule.patch_size, datamodule.batch_size, datamodule.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "127fc622-d6a5-43a0-9497-503774b98b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd62eb3b-8b17-4602-8806-18f03109ea7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3774, 3774)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datamodule.train_batch_sampler), len(datamodule.val_sampler), len(datamodule.test_sampler)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "execution": {
   "timeout": 1200
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
